import os
import json
import argparse
import re
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch
from supabase import create_client, Client
from dotenv import load_dotenv

load_dotenv()

# Supabase setup
url: str = os.environ.get("SUPABASE_URL") or os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
key: str = os.environ.get("SUPABASE_SERVICE_ROLE_KEY") or os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")

if not url or not key:
    raise ValueError("Supabase credentials not found in environment variables")

supabase: Client = create_client(url, key)

# Gemini setup
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY not found in environment variables")

client = genai.Client(api_key=GEMINI_API_KEY)
model_id = "gemini-2.5-flash"
google_search_tool = Tool(
    google_search = GoogleSearch()
)

def get_topics_needing_comment(limit=10, force=False, threshold=3):
    query = supabase.from_("mvp2_megatopics").select("id, title_ko, title_en, ai_summary")
    
    if not force:
        query = query.is_("editor_comment", "null")
        
    query = query.order("created_at", desc=True).limit(limit * 5)
    
    response = query.execute()
    
    topics = []
    for t in response.data:
        # Check articles to count countries
        articles = get_global_topic_context(t['id'])
        if not articles:
            continue
            
        countries = set()
        for a in articles:
            if a.get('country_code'):
                countries.add(a.get('country_code'))
        
        print(f"Topic: {t.get('title_ko')}, Calculated Countries: {len(countries)} ({countries})")
        
        if len(countries) >= threshold:
            t['articles'] = articles # Cache articles to avoid re-fetching
            topics.append(t)
            if len(topics) >= limit:
                break
                
    return topics

def get_global_topic_context(global_topic_id):
    # Fetch related articles (limit 30 for context)
    response = supabase.from_("mvp2_articles")\
        .select("title_ko, title_original, source_name, country_code")\
        .eq("global_topic_id", global_topic_id)\
        .limit(30)\
        .execute()
        
    return response.data

def generate_editor_comment(topic_title, articles, ai_summary=""):
    headlines_by_country = {}
    for a in articles:
        cc = a.get('country_code') or "Unknown"
        title = a.get('title_ko') or a.get('title_original')
        if cc not in headlines_by_country:
            headlines_by_country[cc] = []
        headlines_by_country[cc].append(title)
        
    context_str = ""
    for cc, titles in headlines_by_country.items():
        context_str += f"\n[{cc}]\n" + "\n".join([f"- {t}" for t in titles])
    
    prompt = f"""
    ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ë‰´ìŠ¤ íë ˆì´ì…˜ ì„œë¹„ìŠ¤ 'ë‰´ìŠ¤ ìŠ¤íŽ™íŠ¸ëŸ¼'ì˜ ë©”ì¸ ì—ë””í„°ìž…ë‹ˆë‹¤.
    ë‹¹ì‹ ì˜ íŽ˜ë¥´ì†Œë‚˜ëŠ” **'ì„¸ìƒ ëŒì•„ê°€ëŠ” ì¼ì— ë°ê³ , ìœ„íŠ¸ ìžˆëŠ”, ì¹œí•œ ì¹œêµ¬'**ìž…ë‹ˆë‹¤.
    ë” ìŠ¤í‚¤ë¨¸(The Skimm)ë‚˜ ë‰´ë‹‰(NEWNEEK)ì²˜ëŸ¼ **ì‰½ê³ , ìž¬ë°Œê³ , ì«€ë“í•œ ë¬¸ì²´**ë¥¼ êµ¬ì‚¬í•©ë‹ˆë‹¤.
    

    ì•„ëž˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬, ë…ìžê°€ ì‰½ê³  ìž¬ë¯¸ìžˆê²Œ ì½ì„ ìˆ˜ ìžˆëŠ” ê³ í’ˆì§ˆì˜ ì—ë””í„° ë¶„ì„ê¸€ì„ ìž‘ì„±í•´ì£¼ì„¸ìš”.
    
    [ë¶„ì„ ëŒ€ìƒ í† í”½]: {topic_title}
    
    [AI ìš”ì•½ (ì°¸ê³ ìš©)]:
    {ai_summary}

    [ê´€ë ¨ ê¸°ì‚¬ í—¤ë“œë¼ì¸]:
    {context_str}

    ====================================================
    ðŸš« ì ˆëŒ€ ì–´ê¸°ë©´ ì•ˆ ë˜ëŠ” ê·œì¹™ (Strict Rules)
    ====================================================
    1. **í—¤ë” ê´„í˜¸ ì‚¬ìš© ê¸ˆì§€:**
       - êµ­ê°€ë³„ í—¤ë” ìš”ì•½ë¬¸ì— **ì ˆëŒ€ ê´„í˜¸()ë¥¼ ë„£ì§€ ë§ˆì„¸ìš”.** `|` ë’¤ì— ë°”ë¡œ ë¬¸ìž¥ì„ ì“°ì„¸ìš”.
       - âŒ `## ðŸ‡°ðŸ‡· í•œêµ­ | (ì•ˆíƒ€ê¹Œìš´ ì†Œì‹)` -> â­•ï¸ `## ðŸ‡°ðŸ‡· í•œêµ­ | ì•ˆíƒ€ê¹Œìš´ ì‚¬ê³  ì†Œì‹ì´ ë“¤ë ¤ì™”ì–´ìš”`

    2. **ì†ë§ˆìŒ ë¬¸ìž¥í™”:**
       - 'ë¹„íŒ', 'ì˜¹í˜¸' ë‹¨ì–´ ê¸ˆì§€. **ê·¸ ë‚˜ë¼ì˜ ìž…ìž¥ì„ ëŒ€ë³€í•˜ëŠ” êµ¬ì–´ì²´ ë¬¸ìž¥**ì„ ì“°ì„¸ìš”.
       - âŒ `ðŸ‡ºðŸ‡¸(ë¹„íŒ)` -> â­•ï¸ `ðŸ‡ºðŸ‡¸("ì´ê±° ì§„ì§œ ìœ„í—˜í•œ ê±° ì•„ëƒ?")`

    3. **ì´ëª¨ì§€ ì œì–´:**
       - **ê³ ìŠ´ë„ì¹˜(ðŸ¦”) ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€.**
       - ë¬¸ìž¥ ì¤‘ê°„/ëì— ìž¥ì‹ìš© ì´ëª¨ì§€(ðŸ˜Š, ðŸ˜¢) ê¸ˆì§€. í…ìŠ¤íŠ¸ë¡œë§Œ ë‹´ë°±í•˜ê²Œ.
       - (êµ­ê¸°, ì„¹ì…˜ ì•„ì´ì½˜ì€ í—ˆìš©)

    4. **í†¤ì•¤ë§¤ë„ˆ ì°¨ë³„í™”:**
       - ì¼ë°˜ í† í”½: "ê·¸ê±° ë“¤ì—ˆì–´?", "~ë”ë¼ê³ ìš”" (ì¹œê·¼í•œ ëŒ€í™”ì²´)
       - **ì‚¬ê±´/ì‚¬ê³ /ë²”ì£„:** "ì‚¬ìƒìžê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", "ë…¼ëž€ì´ ë˜ê³  ìžˆìŠµë‹ˆë‹¤" (ê±´ì¡°í•˜ê³  ì°¨ë¶„í•œ ë‰´ìŠ¤ í†¤)

    5. **ì¸ìš©ì˜ íˆ¬ëª…ì„± (Source Attribution):**
       - ðŸ‡·ðŸ‡ºëŸ¬ì‹œì•„ ë§¤ì²´ê°€ ðŸ‡ºðŸ‡¦ìš°í¬ë¼ì´ë‚˜ ì†Œì‹ì„ ì „í•  ë•Œ ë“±, ë§¤ì²´ êµ­ì ê³¼ ë‚´ìš©ì˜ êµ­ì ì´ ë‹¤ë¥´ë©´ "ëŸ¬ì‹œì•„ ì–¸ë¡ ì´ ì¸ìš©í•œ ìš°í¬ë¼ì´ë‚˜ì˜ ìž…ìž¥ì€~" ì²˜ëŸ¼ ì¶œì²˜ ê´€ê³„ë¥¼ ëª…í™•ížˆ ë°ížˆì„¸ìš”.

    6. **ë¬¸ë‹¨êµ¬ì„±:**
       - í•œ ë¬¸ë‹¨ì€ 3ë¬¸ìž¥ì´í•˜ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”. ë¬¸ë‹¨ ê°„ ì¤„ë°”ê¿ˆì€ 2ë²ˆ í•´ì£¼ì„¸ìš”.

    ====================================================
    ðŸ“ ìž‘ì„± êµ¬ì¡° (Output Structure)
    ====================================================


    ### ì—ë””í„°ì˜ ì‹œì„  ðŸ§

    **"ì—¬ê¸°ì— í˜¸ê¸°ì‹¬ì„ ìžê·¹í•˜ëŠ” ë‚šì‹œì„± ë¶€ì œ ìž‘ì„±"**
    (ì´ ì´ìŠˆê°€ ì™œ í•«í•œì§€ ë°°ê²½ ì„¤ëª…. ë¬¸ìž¥ ë‚´ ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€)


    **âš¡ ê²°ì •ì  ì°¨ì´** 
        êµ­ê°€1ì´ëª¨ì§€("ì§§ì€ ì†ë§ˆìŒ ë¬¸ìž¥") 
    vs êµ­ê°€2ì´ëª¨ì§€("ì§§ì€ ì†ë§ˆìŒ ë¬¸ìž¥") 
    vs êµ­ê°€3ì´ëª¨ì§€("ì§§ì€ ì†ë§ˆìŒ ë¬¸ìž¥")

    (ìƒì„¸ ì„¤ëª…: "êµ­ê°€1ì€ ~ë¼ê³  ê±±ì •í•˜ëŠ”ë°, êµ­ê°€2ëŠ” ì˜¤ížˆë ¤ ~ë¼ë©° ë°˜ê¸°ëŠ” ë¶„ìœ„ê¸°ì˜ˆìš”. ê·¸ ì´ìœ ëŠ”...", ë¬¸ìž¥ ë‚´ ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€)
"""

    try:
        response = client.models.generate_content(
            model=model_id,
            contents=prompt,
            config=GenerateContentConfig(
                tools=[google_search_tool]
            )
        )
        return response.text
    except Exception as e:
        print(f"âŒ Gemini API Error for {topic_title}: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description="Generate Editor's Perspective for global topics")
    parser.add_argument("--limit", type=int, default=5, help="Number of topics to process")
    parser.add_argument("--force", action="store_true", help="Reprocess topics that already have comments")
    parser.add_argument("--all", action="store_true", help="Process all topics in batches")
    parser.add_argument("--threshold", type=int, default=1, help="Minimum number of countries required")
    args = parser.parse_args()

    if args.all:
        print(f"Processing ALL eligible global topics (>={args.threshold} countries) in batches...")
        while True:
            topics = get_topics_needing_comment(args.limit, args.force, args.threshold)
            if not topics:
                print("No more topics to process.")
                break
            
            print(f"Processing batch of {len(topics)} topics...")
            process_batch(topics)
    else:
        topics = get_topics_needing_comment(args.limit, args.force, args.threshold)
        print(f"Found {len(topics)} eligible topics to process")
        process_batch(topics)

def process_batch(topics):
    for topic in topics:
        title = topic.get('title_ko') or topic.get('title_en')
        ai_summary = topic.get('ai_summary') or ""
        print(f"\nProcessing: {title}")
        
        articles = topic.get('articles')
        if not articles:
            articles = get_global_topic_context(topic['id'])
        
        if not articles:
            print("  - No related articles found, skipping")
            continue
            
        comment = generate_editor_comment(title, articles, ai_summary)
        
        if comment:
            print("  - Comment generated, updating DB...")
            supabase.from_("mvp2_megatopics").update({"editor_comment": comment}).eq("id", topic['id']).execute()
            print("  - Done")
            print("\n--- Generated Output ---\n")
            print(comment)
        else:
            print("  - Failed to generate comment")

if __name__ == "__main__":
    main()
